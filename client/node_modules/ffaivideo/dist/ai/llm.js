"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.callAIInterface = void 0;
const g4f_1 = require("g4f");
const openai_1 = __importDefault(require("openai"));
const axios_1 = __importDefault(require("axios"));
const gpt4js_1 = __importDefault(require("gpt4js"));
const lodash_1 = require("lodash");
const azure_openai_1 = require("azure-openai");
const generative_ai_1 = require("@google/generative-ai");
const log_1 = require("../utils/log");
const getLLMConfig = (config) => {
    const provider = config.provider.toLowerCase();
    let llmConfig = { ...config.default };
    const providerConfigs = {
        moonshot: config.moonshot,
        openai: config.openai,
        azure: config.azure,
        gemini: config.gemini,
        g4f: config.g4f,
        gpt4js: config.gpt4js,
        custom: config.customAI,
    };
    if (provider in providerConfigs) {
        const providerConfig = providerConfigs[provider];
        if (providerConfig) {
            llmConfig = { ...llmConfig, ...providerConfig };
        }
    }
    else {
        throw new Error('LLM provider is not set in the config file.');
    }
    if (provider !== 'custom') {
        const requiredFields = ['apiKey', 'modelName', 'baseUrl'];
        for (const field of requiredFields) {
            if (!llmConfig[field]) {
                throw new Error(`${provider}: ${field} is not set in the config file.`);
            }
        }
    }
    return llmConfig;
};
const callAIInterface = async (prompt, config) => {
    var _a, _b, _c;
    let content = '';
    const llmConfig = getLLMConfig(config);
    const provider = config.provider.toLowerCase();
    if (provider === 'g4f') {
        const g4f = new g4f_1.G4F();
        const modelName = config.g4f.modelName;
        const messages = [{ role: 'user', content: prompt }];
        try {
            content = await g4f.chatCompletion(messages, {
                model: modelName,
            });
        }
        catch (e) {
            log_1.Logger.log(`Sorry, there was an error calling g4f.`, e);
        }
        return content;
    }
    if (provider === 'gpt4js') {
        const providerName = ((_a = config.gpt4js) === null || _a === void 0 ? void 0 : _a.provider) || 'Nextway';
        const gpt4js = await (0, gpt4js_1.default)();
        const provider = gpt4js.createProvider(providerName);
        const modelName = config.gpt4js.modelName;
        const messages = [{ role: 'user', content: prompt }];
        try {
            content = await provider.chatCompletion(messages, {
                provider: providerName,
                model: modelName,
            }, (data) => { });
        }
        catch (e) {
            log_1.Logger.log(`Sorry, there was an error calling gpt4js.`, e);
        }
        return content;
    }
    if (provider === 'gemini') {
        const generationConfig = {
            temperature: 0.5,
            top_p: 1,
            top_k: 1,
            max_output_tokens: 2048,
        };
        const safetySettings = [
            {
                category: generative_ai_1.HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold: generative_ai_1.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            },
            {
                category: generative_ai_1.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold: generative_ai_1.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            },
            {
                category: generative_ai_1.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                threshold: generative_ai_1.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            },
            {
                category: generative_ai_1.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                threshold: generative_ai_1.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            },
        ];
        const genAI = new generative_ai_1.GoogleGenerativeAI(llmConfig.apiKey);
        const model = genAI.getGenerativeModel({
            model: llmConfig.modelName || '',
            generationConfig,
            safetySettings,
        });
        const chat = model.startChat();
        try {
            const result = await chat.sendMessage(prompt);
            content = result.response.text();
        }
        catch (e) {
            log_1.Logger.log(`Sorry, there was an error calling gemini.`, e);
        }
        return content;
    }
    if (provider === 'azure') {
        const client = new azure_openai_1.OpenAIApi(new azure_openai_1.Configuration({
            apiKey: llmConfig.apiKey,
            basePath: llmConfig.baseUrl,
        }));
        const response = await client.createCompletion({
            model: llmConfig.modelName,
            prompt: prompt,
            max_tokens: 100,
            top_p: 1,
            frequency_penalty: 0,
            presence_penalty: 0,
        });
        content = response.data.choices[0].text.trim();
        return content;
    }
    if (provider === 'custom') {
        try {
            const data = { message: prompt, ...llmConfig.data };
            const response = await axios_1.default.post((_b = llmConfig.baseUrl) !== null && _b !== void 0 ? _b : '', data, llmConfig.requestConfig);
            if (response && response.data) {
                const responsePath = llmConfig.responsePath || 'data.choices[0].message.content';
                content = (0, lodash_1.get)(response.data, responsePath, '');
            }
            return content;
        }
        catch (error) {
            console.error('Error making custom API call:', error);
            throw error;
        }
    }
    log_1.Logger.log(provider, JSON.stringify(llmConfig));
    const client = new openai_1.default({
        apiKey: llmConfig.apiKey,
        baseURL: llmConfig.baseUrl,
    });
    const response = await client.chat.completions.create({
        model: llmConfig.modelName,
        messages: [{ role: 'user', content: prompt }],
    });
    if (response) {
        content = ((_c = response.choices[0].message) === null || _c === void 0 ? void 0 : _c.content) || '';
    }
    return content.replace('\n', '');
};
exports.callAIInterface = callAIInterface;
//# sourceMappingURL=llm.js.map